\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{listings}

\lhead{Intro to Parallel Comp PA3}
\chead{Melvyn Ian Drag}
\rhead{\today}
\setlength{\parskip}{0pt} 
\setlength{\parindent}{0pt}

\begin{document}
\begin{enumerate}
\item The following analysis is done for a thread, not for the program as a whole.

\begin{enumerate}
\item
\begin{lstlisting}
extern "C" __global__ void
mmkernel( float* a, float* b, float* c,
  int pitch_a, int pitch_b, int pitch_c,
  int n, int m, int p )
{
    int i = blockIdx.x*32 + threadIdx.x;
    int j = blockIdx.y;

    float sum = 0.0;
    for( int k = 0; k < p; ++k ) 
      sum += b[i+pitch_b*k] * c[k+pitch_c*j];
    a[i+pitch_a*j] = sum; 
}
\end{lstlisting}

Each thread will perform $2P$ FLOPs and will access global memory $2P + 1$ times, giving a ratio of about $\frac11$.

\item
\begin{lstlisting}
extern "C" __global__ void
mmkernel( float* a, float* b, float* c,
  int pitch_a, int pitch_b, int pitch_c,
  int n, int m, int p )
{
    int tx = threadIdx.x;
    int i = blockIdx.x*32 + tx;
    int j = blockIdx.y;
    __shared__ float cb[32];

    float sum = 0.0;
    for( int ks = 0; ks < p; ks += 32 ){
      cb[tx] = c[ks+tx+pitch_c*j];
      // Why dont we need a __syncthreads() here ?
      for( int k = ks; k < ks+32; ++k )
        sum += b[i+pitch_b*k] * cb[k-ks];
    }
    a[i+pitch_a*j] = sum;
}
\end{lstlisting}	

There will be less global memory access due to the introduction of cb. A thread will still perform $2P$ FLOPs. However, a thread will make $\frac{P}{32}$ accesses to global memory when accessing c, $P$ accesses to global memory when accessing b, and one global memory access when accessing a. So the ration is $\frac{\frac{P}{32} + P + 1}{2P} \approx \frac12$.

\item
\begin{lstlisting}
extern "C" __global__ void
mmkernel( float* a, float* b, float* c,
  int pitch_a, int pitch_b, int pitch_c,
  int n, int m, int p )
{
    int tx = threadIdx.x;
    int i = blockIdx.x*64 + tx;
    int j = blockIdx.y;
    __shared__ float cb[32];

    float sum0 = 0.0, sum1 = 0.0;
    for( int ks = 0; ks < p; ks += 32 ){
      cb[tx] = c[ks+tx+pitch_c*j];
      __syncthreads();
      for( int k = ks; k < ks+32; ++k ){
        sum0 += b[i+pitch_b*k] * cb[k-ks];
        sum1 += b[i+32+pitch_b*k] * cb[k-ks];
      }
      __syncthreads();
    }
    a[i+pitch_a*j] = sum0;
    a[i+32+pitch_a*j] = sum1;
}
\end{lstlisting}

Every thread will perform $4P$ FLOPs . There are 2 global memory accesses due to a, $\frac{P}{32}$ global memory accesses due to c, and $2P$ global memory accesses due to b. The ratio is $\frac{2P + \frac{P}{32} + 2}{4P} \approx \frac12$. We should expect better performance than k2, however, due to the loop unrolling.

\item
\begin{lstlisting}
extern "C" __global__ void
mmkernel( float* a, float* b, float* c,
  int pitch_a, int pitch_b, int pitch_c,
  int n, int m, int p )
{
    int tx = threadIdx.x;
    int i = blockIdx.x*32 + tx;
    int j = blockIdx.y*2;
    __shared__ float cb0[32], cb1[32];

    float sum0 = 0.0, sum1 = 0.0;
    for( int ks = 0; ks < p; ks += 32 ){
      cb0[tx] = c[ks+tx+pitch_c*j];
      cb1[tx] = c[ks+tx+pitch_c*(j+1)];
      __syncthreads();
      for( int k = ks; k < ks+32; ++k ){
        float rb = b[i+pitch_b*k];
        sum0 += rb * cb0[k-ks];
        sum1 += rb * cb1[k-ks];
      }
      __syncthreads();
    }
    a[i+pitch_a*j] = sum0;
    a[i+pitch_a*(j+1)] = sum1;
}
\end{lstlisting}				

A thread running this version of the kernel will perform $4P$ FLOPs, $2\frac{P}{32}$ global memory accesses due to c, $P$ global memory accesses due to b, and 2 global memory accesses due to a. The ratio is $\frac{\frac{P}{16} + P + 2}{4P}\approx\frac14$. 
\end{enumerate}

We expect the runtime of the kernels to be decreasing. If we assume that the ratio of global memory access to FLOPs is the only expensive transaction occuring during the run of the kernel, then we will expect k1 to have a performance of $x$ GLOPS, and k2 and k3 to both produce $2x$ GLOPS, and then k4 to produce $4x$ GFLOPS. We do not see this performance improvement. Evidently, global memory access is not the only consideration to be made when optimizing kernel codes. Note that k4 does nto even double to performance of k1.

\textbf{RESULTS}
\begin{lstlisting}
-bash-4.1$ ./mmdriver -bin k1.bin -block 32 1024 -thread  32 1 -mat 1024 -size 1024 -check
binfile=k1.bin  array=1024x1024  matrix=1024x1024  block=<32x1024>  thread=<32x1>
matrix = 1024x1024
 array = 1024x1024
  grid = 32x1024
 block = 32x1x1
 flops = 2147483648
  msec =      87996   GFLOPS =   24.40,   26.49 (kernel)
no errors found

-bash-4.1$ ./mmdriver -bin k2.bin -block 32 1024 -thread  32 1 -mat 1024 -size 1024 -check
binfile=k2.bin  array=1024x1024  matrix=1024x1024  block=<32x1024>  thread=<32x1>
matrix = 1024x1024
 array = 1024x1024
  grid = 32x1024
 block = 32x1x1
 flops = 2147483648
  msec =      94413   GFLOPS =   22.75,   24.59 (kernel)
no errors found

-bash-4.1$ ./mmdriver -bin k3.bin -block 16 1024 -thread  32 1 -mat 1024 -size 1024 -check
binfile=k3.bin  array=1024x1024  matrix=1024x1024  block=<16x1024>  thread=<32x1>
matrix = 1024x1024
 array = 1024x1024
  grid = 16x1024
 block = 32x1x1
 flops = 2147483648
  msec =      53414   GFLOPS =   40.20,   46.06 (kernel)
no errors found

-bash-4.1$ ./mmdriver -bin k4.bin -block 32 512 -thread  32 1 -mat 1024 -size 1024 -check
binfile=k4.bin  array=1024x1024  matrix=1024x1024  block=<32x512>  thread=<32x1>
matrix = 1024x1024
 array = 1024x1024
  grid = 32x512
 block = 32x1x1
 flops = 2147483648
  msec =      52761   GFLOPS =   40.70,   47.17 (kernel)
no errors found
\end{lstlisting}
\item
\item 
\end{enumerate}
\end{document}
